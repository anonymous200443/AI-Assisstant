
---

# ğŸŒ Real-Time Multilingual AI Assistant ğŸ¤–

A blazing-fast AI assistant that supports **real-time text, voice, and image input** with **multilingual output**, powered by **Groq LPU inference**, **FastAPI**, and **React**.
View Website at https://aiassisstant.netlify.app/

---

## ğŸš€ Features

- ğŸ’¬ Real-time multilingual chat (text & voice)
- ğŸ¤ Speech recognition & speech synthesis
- ğŸ–¼ï¸ Image text recognition (OCR) using Tesseract.js
- ğŸ§  Context-aware responses with memory
- ğŸŒ Automatic language detection
- ğŸ” Suggested replies & auto-scroll
- ğŸŒ™ Dark mode toggle
- ğŸ—£ï¸ Voice output toggle
- ğŸ“‹ Copy-to-clipboard button
- ğŸ§¹ Clear chat functionality
- âš¡ Ultra-fast inference with Groq
- ğŸ§ª Lottie animation bot avatar
- ğŸ”„ Loading spinner for bot responses

---

## ğŸ“¸ Demo

![Demo Screenshot](./demo-screenshot.png)

---

## ğŸ›  Technologies Used

**Frontend**:  
React, JavaScript, Lottie, TailwindCSS (or your CSS choice), react-speech-recognition, Tesseract.js, Axios, Framer Motion

**Backend**:  
FastAPI, Python, Uvicorn, Groq LPU Inference API, TTS, Langchain (if used), OpenAI-compatible format

---

## ğŸ“¦ Installation

### ğŸ”§ Backend Setup

1. Clone the repo  
   ```bash
   git clone https://github.com/your-username/ai-assistant-groq.git
   cd backend
   ```

2. Create and activate a virtual environment  
   ```bash
   python -m venv venv
   source venv/bin/activate  # or venv\Scripts\activate on Windows
   ```

3. Install dependencies  
   ```bash
   pip install -r requirements.txt
   ```

4. Start the FastAPI server  
   ```bash
   uvicorn main:app --reload
   ```

> âœ… Make sure your backend binds to `0.0.0.0` if deploying on Render or similar.

---

### ğŸ§‘â€ğŸ’» Frontend Setup

1. Go to the frontend folder  
   ```bash
   cd ../frontend
   ```

2. Install dependencies  
   ```bash
   npm install
   ```

3. Start the development server  
   ```bash
   npm start
   ```

---

## ğŸ“„ How to Use

- Type or speak your message using the mic button.
- Receive real-time responses from the AI.
- Upload images to extract text.
- Toggle between dark/light mode, language, and voice output.
- Copy bot replies or clear chat history.

---

## ğŸ§© Folder Structure

```
ai-assistant-groq/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ App.jsx
â”‚   â””â”€â”€ public/
â”‚       â””â”€â”€ index.html
â””â”€â”€ README.md
```

---

## ğŸ§  Challenges I Faced

- Managing consistent voice output across languages with Web Speech API
- Deploying backend with proper port binding for platforms like Render
- Integrating Tesseract OCR without blocking the UI thread
- Handling automatic language detection with user preferences

---

## âœ¨ Future Enhancements

- User authentication & conversation history
- File and document support
- Emotion detection for voice/video
- Push-to-talk and whisper-to-respond modes
- Mobile-first PWA version

---

## ğŸ“¬ Feedback & Contributions

PRs and feature suggestions are welcome! Feel free to fork and build your own version.

---

## ğŸ§  Powered by Groq

Groqâ€™s blazing-fast LPU inference unlocks the power of large language models in real-time across modalities. This project is built to showcase Groq's ability to handle multilingual and multimodal AI assistants with speed and efficiency.

---

